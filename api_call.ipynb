{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "import os\n",
    "import csv\n",
    "\n",
    "PATH = os.getcwd()\n",
    "def read_file_from_csv(filename):\n",
    "    # source = '%s%s/%s' % (PATH, pathname, filename)\n",
    "    df = None\n",
    "    try:\n",
    "        df = pd.read_csv(filename, parse_dates=True, infer_datetime_format=True, na_filter=True, quoting=csv.QUOTE_ALL, thousands=',')\n",
    "        print(\"read\",len(df.index),\" rows from\", filename)\n",
    "    except:\n",
    "        print(\"error opening the file %s\" % filename)\n",
    "    finally:\n",
    "        return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API call successful. Generating files to match\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "clientid = 'test'\n",
    "N = 5\n",
    "url = 'https://zgvbsyp62d.execute-api.us-east-2.amazonaws.com/dev/matches_broker-dev?clientid={}&N={}'.format(clientid, N)\n",
    "\n",
    "r = requests.post(url)\n",
    "if r.status_code == 200:\n",
    "    print('API call successful. Generating files to match')\n",
    "    data = r.text\n",
    "    data = json.loads(data)\n",
    "    files_to_match = data['matches']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rj/rrq458bj1mb9fpw62rmd9yb80000gn/T/ipykernel_3983/2428096568.py:3: DtypeWarning: Columns (62) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  matches = read_file_from_csv(\"files/matches.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 75313  rows from files/matches.csv\n"
     ]
    }
   ],
   "source": [
    "# Get names\n",
    "import pandas as pd\n",
    "matches = read_file_from_csv(\"files/matches.csv\")\n",
    "\n",
    "matches[\"source_UPC\"] = matches[\"source_UPC\"].fillna('')\n",
    "matches[\"source_UPC\"] = matches[\"source_UPC\"].astype(str) \n",
    "matches['source_UPC'] = matches.source_UPC.str.strip()\n",
    "\n",
    "matches[\"dest_UPC\"] = matches[\"dest_UPC\"].fillna('')\n",
    "matches[\"dest_UPC\"] = matches[\"dest_UPC\"].astype(str) \n",
    "matches['dest_UPC'] = matches.dest_UPC.str.strip()\n",
    "\n",
    "# Get product names where source UPC/dest UPC is null\n",
    "missing_source = matches[(matches['source_UPC'] == '') & (matches['source_supplier'] != 'upcitemdb-offer') & (matches['source_supplier'] != 'upcitemdb')]\n",
    "missing_source_list = missing_source['source_name'].to_list()\n",
    "\n",
    "missing_dest= matches[(matches['dest_UPC'] == '')  & (matches['dest_supplier'] != 'upcitemdb-offer') & (matches['dest_supplier'] != 'upcitemdb')]\n",
    "missing_dest_list = missing_dest['dest_name'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23736\n",
      "10071\n"
     ]
    }
   ],
   "source": [
    "# Combine the 2 and dedupe on names\n",
    "missing_upcs = missing_source_list + missing_dest_list\n",
    "missing_upcs = [x.strip() for x in missing_upcs]\n",
    "print(len(missing_upcs))\n",
    "missing_upcs = list(set(missing_upcs))\n",
    "print(len(missing_upcs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter out names. Pick only those which are not lookedup yet\n",
    "f = open('files/lookedup')\n",
    "lookedup_names = f.readlines()\n",
    "lookedup_names = [l.strip() for l in lookedup_names]\n",
    "lookedup_names = {l:0 for l in lookedup_names}\n",
    "\n",
    "missing_upcs = [m for m in missing_upcs if m not in lookedup_names]\n",
    "len(missing_upcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fashnpoint White One Sixth Fold Ultra Ply Paper Guest Towel',\n",
       " '- TurnburyÂ® Insulated Pedestal Based Bowl 5 oz (48/cs) - Onyx',\n",
       " 'MICRO ITALIAN BASIL  4oz/EA',\n",
       " 'MCCORMICK CULINARY POPPY SEED',\n",
       " '100 Percentage PCW Paper Compostable Hot Cup Sleeve - 8 Oz.']"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transformations\n",
    "og_lookup_names = [n for n in missing_upcs]\n",
    "tf_lookup_names = [n for n in missing_upcs]\n",
    "tf_lookup_names = [l.lower() for l in tf_lookup_names]\n",
    "\n",
    "# Remove oz \n",
    "tf_lookup_names = [l.replace('oz.', '') for l in tf_lookup_names]\n",
    "#tf_lookup_names[:10]\n",
    "\n",
    "og_lookup_names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('files/out')\n",
    "items_dic = json.load(f)\n",
    "len(items_dic)\n",
    "\n",
    "#items_dic = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 items for toilet bowl mop\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "headers = {'user_key' : 'f112765d522453bdb002e9b928f7ab1d'}\n",
    "# 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:77.0) Gecko/20190101 Firefox/77.0'}\n",
    "for n, for_url in zip(og_lookup_names[0:], tf_lookup_names[0:]):\n",
    "    if n not in items_dic or items_dic[n] == 429:\n",
    "        try:\n",
    "            url = 'https://api.upcitemdb.com/prod/v1/search?s={}&match_mode=1&type=product'.format(for_url)\n",
    "            x = requests.get(url, headers=headers, timeout=15)\n",
    "\n",
    "            data = x.json()\n",
    "            list_items = []\n",
    "            for i in data['items']:\n",
    "                list_items.append(i)\n",
    "            items_dic[n] = list_items\n",
    "            print('Found', len(list_items), 'items for', for_url)\n",
    "        except:\n",
    "            print('No match found for:', for_url, 'Error code:', x.status_code)\n",
    "            items_dic[n] = x.status_code\n",
    "        time.sleep(5.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2247\n"
     ]
    }
   ],
   "source": [
    "print(len(items_dic))\n",
    "# Export result\n",
    "import json\n",
    "with open('files/out', 'w') as fout:\n",
    "    json.dump(items_dic, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export names that are already searched\n",
    "with open('files/lookedup', 'a') as fout:\n",
    "    fout.write(\"\\n\".join(og_lookup_names))\n",
    "    fout.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export names with 404 error\n",
    "error_404 = []\n",
    "for k,v in items_dic.items():\n",
    "    if type(v) is not list:\n",
    "        error_404.append(k)\n",
    "\n",
    "f = open('files/error_404')\n",
    "_tmp = f.readlines()\n",
    "_tmp = [l.strip() for l in _tmp]\n",
    "_tmp = {l:0 for l in _tmp}\n",
    "\n",
    "error_404 = [e for e in error_404 if e not in _tmp]\n",
    "\n",
    "with open('files/error_404', 'a') as fout:\n",
    "    fout.write(\"\\n\".join(error_404))\n",
    "    fout.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore available columns in data\n",
    "from datetime import datetime\n",
    "\n",
    "columns = ['lookup_name', 'product_url', 'price', 'is_offer']\n",
    "\n",
    "for k,v in items_dic.items():\n",
    "    if type(v) is list:\n",
    "        for _ in v:\n",
    "            for _k, _v in _.items():\n",
    "                if _k not in columns and _k != 'offers':\n",
    "                    columns.append(_k)\n",
    "\n",
    "columns.append('updated_t')\n",
    "# Create item list and equivalent csv\n",
    "results = []\n",
    "for k,v in items_dic.items():\n",
    "    if type(v) is list:\n",
    "        for _ in v:\n",
    "            default_dic = {}\n",
    "            for c in columns:\n",
    "                default_dic[c] = ''\n",
    "            default_dic['updated_t'] = datetime.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            default_dic['lookup_name'] = k\n",
    "            for _k, _v in _.items():\n",
    "                if _k != 'offers':\n",
    "                    default_dic[_k] = _v\n",
    "            results.append(dict(default_dic))\n",
    "            for o in _['offers']:\n",
    "                default_dic['product_url'] = o['link']\n",
    "                default_dic['title'] = o['title']\n",
    "                default_dic['price'] = o['price']\n",
    "                default_dic['is_offer'] = 1\n",
    "                ts = int(o['updated_t'])\n",
    "                # if you encounter a \"year is out of range\" error the timestamp\n",
    "                # may be in milliseconds, try `ts /= 1000` in that case\n",
    "                default_dic['updated_t'] = datetime.utcfromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                results.append(dict(default_dic))\n",
    "        \n",
    "\n",
    "enriched = pd.DataFrame(results)\n",
    "enriched.to_csv('files/enriched.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "for k,v in items_dic.items():\n",
    "    if v == 404:\n",
    "        c+=1\n",
    "c\n",
    "len(og_lookup_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in items_dic.items():\n",
    "    if type(v) is list:\n",
    "        print(v)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random\n",
    "# x = re.findall(r\"[ \\t]*(?:\\d|[^\\w\\s])+[ \\t]*\", x) # Extract the numeric parts\n",
    "\n",
    "import re\n",
    "a = 'Gojo, GOJ965212CT, Instant Hand Sanitizer, 12 / Carton, Clear'\n",
    "#a = 'AJM Packaging Corporation White Paper Plates, 9\" Diameter, 100 Count'\n",
    "#a = 'Keebler Club Crackers Original .25oz'\n",
    "a = 'Package Size: 12x 6oz'\n",
    "look_ahead = \"gallon|each|oz|ml|lb|gram|CT|oz|fl oz|\\\"\"\n",
    "\n",
    "ss = '(\\d+(?:\\.\\d*)?\\\\s?(?=({})(?:\\\\s|$|,)))'.format(look_ahead)\n",
    "\n",
    "ss = '((?:^|\\s)\\d*\\.?\\d+?\\\\s?(?=({})(?:\\\\s|$|,)))'.format(look_ahead)\n",
    "\n",
    "\n",
    "\n",
    "print(re.findall(ss, a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo(x):\n",
    "        x = x.lower()\n",
    "        if 'pack of' in x:\n",
    "            x = x.split('pack of')\n",
    "            print(re.findall('\\d*\\.?\\d+?', x[1]))\n",
    "            try:\n",
    "                return int(re.findall('\\d*\\.?\\d+?', x[1])[0])\n",
    "            except:\n",
    "                ''\n",
    "        elif 'pack' in x:\n",
    "            x = x.split('pack')\n",
    "            try:\n",
    "                return int(re.findall('\\d*\\.?\\d+?', x[0])[0])\n",
    "            except:\n",
    "                ''\n",
    "        return ''\n",
    "a='8 Ounce (Pack of 1)'\n",
    "foo(a)\n",
    "#re.findall('\\d*\\.?\\d+?', 'asd (23.3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= '''\n",
    "Package Size: \n",
    "\t\t\t\t\t\t\t12ct/CS\n",
    "'''\n",
    "\n",
    "x = x.replace('#', '')\n",
    "x = x.split('/')\n",
    "if 'ct' in x[0]:\n",
    "    x = x[0]\n",
    "\n",
    "x.split('ct')[0].strip().split(':')[-1].strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r'\\b\\d+\\b', 'Pack: 1Case')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bbfebc3f73dbbf16f9c58aa6923efdfb584a134391f2827f083ded2d0fd6d71e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('my')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
