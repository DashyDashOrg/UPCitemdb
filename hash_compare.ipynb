{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hassansaeed/Documents/Anaconda/anaconda3/envs/my/lib/python3.9/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from polyfuzz import PolyFuzz\n",
    "\n",
    "import productmatching\n",
    "\n",
    "\n",
    "PATH = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read all-merged and matches file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rj/rrq458bj1mb9fpw62rmd9yb80000gn/T/ipykernel_52121/355239101.py:2: DtypeWarning: Columns (17,18,19,22) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  df_all_merged = productmatching.read_file_from_csv(base_folder + \"analytics/products\", \"all-merged.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 68136  rows from aws/dev-dashy-refined-datasets/analytics/products all-merged.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' df_roundeye = productmatching.read_file_from_csv(base_folder + \"datasets\", \"RoundEye-merged.csv\")\\ndf_foodservice = productmatching.read_file_from_csv(base_folder + \"datasets\", \"FoodServicesDirect-merged.csv\")\\ndf_birite = productmatching.read_file_from_csv(base_folder + \"datasets\", \"BiRite-merged.csv\")\\ndf_warehouse= productmatching.read_file_from_csv(base_folder + \"datasets\", \"115WareHouse-merged.csv\")\\ndf_matches = productmatching.read_file_from_csv(base_folder + \"matching\", \"matches.csv\")\\ndf_inv = productmatching.read_file_from_csv(base_folder + \"datasets\", \"invoices-unique-products.csv\") '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_folder = 'aws/dev-dashy-refined-datasets/'\n",
    "df_all_merged = productmatching.read_file_from_csv(base_folder + \"analytics/products\", \"all-merged.csv\")\n",
    "\"\"\" df_roundeye = productmatching.read_file_from_csv(base_folder + \"datasets\", \"RoundEye-merged.csv\")\n",
    "df_foodservice = productmatching.read_file_from_csv(base_folder + \"datasets\", \"FoodServicesDirect-merged.csv\")\n",
    "df_birite = productmatching.read_file_from_csv(base_folder + \"datasets\", \"BiRite-merged.csv\")\n",
    "df_warehouse= productmatching.read_file_from_csv(base_folder + \"datasets\", \"115WareHouse-merged.csv\")\n",
    "df_matches = productmatching.read_file_from_csv(base_folder + \"matching\", \"matches.csv\")\n",
    "df_inv = productmatching.read_file_from_csv(base_folder + \"datasets\", \"invoices-unique-products.csv\") \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all hashes from all_merged\n",
    "all_merged_hash = df_all_merged['hash'].to_list()\n",
    "#Dedupe if any\n",
    "all_merged_hash = list(set(all_merged_hash))\n",
    "\n",
    "#Get all source hash from matches\n",
    "matches_source_hash = df_matches['source_hash'].to_list()\n",
    "#Dedupe if any\n",
    "matches_source_hash = list(set(matches_source_hash))\n",
    "\n",
    "#Get all destination hash from matches\n",
    "matches_dest_hash = df_matches['dest_hash'].to_list()\n",
    "#Dedupe if any\n",
    "matches_dest_hash = list(set(matches_dest_hash))\n",
    "\n",
    "# common codes\n",
    "#intersection_source = list(set(all_merged_hash).intersection(matches_source_hash))\n",
    "#print(len(intersection_source), len(matches_source_hash))\n",
    "#intersection_dest = list(set(all_merged_hash).intersection(matches_dest_hash))\n",
    "#print(len(intersection_dest), len(matches_dest_hash))\n",
    "\n",
    "# source hashes missing from all merged\n",
    "missing_source = list(set(matches_source_hash).difference(all_merged_hash))\n",
    "print(len(missing_source), 'source hashes missing from all_merged')\n",
    "missing_dest = list(set(matches_dest_hash).difference(all_merged_hash))\n",
    "print(len(missing_dest), 'destination hashes missing from all_merged')\n",
    "\n",
    "# See 10 missing source hashes\n",
    "missing_source[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>total_counts</th>\n",
       "      <th>missing_counts</th>\n",
       "      <th>missing_%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Food Service Direct</td>\n",
       "      <td>16420</td>\n",
       "      <td>1756</td>\n",
       "      <td>10.694275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>birite</td>\n",
       "      <td>13690</td>\n",
       "      <td>8304</td>\n",
       "      <td>60.657414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Webstaurant Store</td>\n",
       "      <td>13399</td>\n",
       "      <td>1626</td>\n",
       "      <td>12.135234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Warehouse 115</td>\n",
       "      <td>9880</td>\n",
       "      <td>41</td>\n",
       "      <td>0.414980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Round Eye Supply</td>\n",
       "      <td>9870</td>\n",
       "      <td>9870</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Others/Invoice</td>\n",
       "      <td>3875</td>\n",
       "      <td>3875</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Chefs Store</td>\n",
       "      <td>1002</td>\n",
       "      <td>1002</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  type  total_counts  missing_counts   missing_%\n",
       "0  Food Service Direct         16420            1756   10.694275\n",
       "1               birite         13690            8304   60.657414\n",
       "2    Webstaurant Store         13399            1626   12.135234\n",
       "3        Warehouse 115          9880              41    0.414980\n",
       "4     Round Eye Supply          9870            9870  100.000000\n",
       "5       Others/Invoice          3875            3875  100.000000\n",
       "6          Chefs Store          1002            1002  100.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_matches[df_matches['source_hash'].isin(missing_source[:10])]\n",
    "pd.set_option('display.max_columns', None)  # or 1000\n",
    "pd.set_option('display.max_rows', None)  # or 1000\n",
    "pd.set_option('display.max_colwidth', None)  # or 199\n",
    "\n",
    "#df_matches[df_matches['source_hash'].isin(missing_source) & (df_matches['notes'] != 'missing')].groupby(['source_file']).size().reset_index(name='counts')\n",
    "def foo(x):\n",
    "    if x in ['Food Service Direct', 'birite', 'Webstaurant Store', 'Warehouse 115'\\\n",
    "        ,'Round Eye Supply', 'Chefs Store' ]:\n",
    "        return x\n",
    "    else:\n",
    "        return 'Others/Invoice'\n",
    "df_all_merged['type'] = df_all_merged['supplier'].apply(foo)\n",
    "_1 = df_all_merged.groupby(['type']).size().reset_index(name='total_counts').sort_values(['total_counts'],ascending=False)\n",
    "_2 = df_all_merged[(df_all_merged['UPC'] == '')].groupby(['type']).size().reset_index(name='missing_counts').sort_values(['missing_counts'],ascending=False)\n",
    "_1 = _1.merge(_2,how='inner',left_on=['type'],right_on=['type'])\n",
    "_1['missing_%'] = 100*_1['missing_counts']/_1['total_counts']\n",
    "_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Round Eye Supply</td>\n",
       "      <td>9870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>birite</td>\n",
       "      <td>8304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Others/Invoice</td>\n",
       "      <td>3875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Food Service Direct</td>\n",
       "      <td>1756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Webstaurant Store</td>\n",
       "      <td>1626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chefs Store</td>\n",
       "      <td>1002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Warehouse 115</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  type  counts\n",
       "3     Round Eye Supply    9870\n",
       "6               birite    8304\n",
       "2       Others/Invoice    3875\n",
       "1  Food Service Direct    1756\n",
       "5    Webstaurant Store    1626\n",
       "0          Chefs Store    1002\n",
       "4        Warehouse 115      41"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change source file to see product details for a specific file\n",
    "df_matches[(df_matches['source_hash'].isin(missing_source)) & \\\n",
    "    (df_matches['notes'] != 'missing') & (df_matches['source_file'] == 'BiRite-merged.csv')][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check product details in all_merged\n",
    "df_all_merged[df_all_merged['name'] == 'BASE CHICKEN GRANULES NO MSG']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check product details in individual file\n",
    "df_birite[df_birite['name'] == 'BASE CHICKEN GRANULES NO MSG']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_roundeye[df_roundeye['name'] == '44 LB Thai Jasmine Rice']\n",
    "#df_foodservice[df_foodservice['name'] == 'Coffee Pro 30-Cup Percolating Urn, Stainless Steel']\n",
    "#df_birite[df_birite['name'] == 'BASE CHICKEN GRANULES NO MSG GLUTENFREE']\n",
    "#df_all_merged[df_all_merged['name'] == 'Maggi Gluten Free Chicken Base (No Added MSG) 25lb.']\n",
    "import hashlib\n",
    "\n",
    "x = 'biriteBASE CHICKEN GRANULES NO MSG GLUTENFREEUPC-07482611303943619'\n",
    "y = 'biriteBASE CHICKEN GRANULES NO MSG GLUTENFREEUPC-07482611303943619.0'\n",
    "\n",
    "print(hashlib.md5(x.encode()).hexdigest(), hashlib.md5(y.encode()).hexdigest())\n",
    "\n",
    "x = 'biriteSAUCE HOT GREEN CHILI PEPPER GLASSUPC-049733950118133818'\n",
    "y = 'biriteSAUCE HOT GREEN CHILI PEPPER GLASSUPC-049733950118133818.0'\n",
    "print(hashlib.md5(x.encode()).hexdigest(), hashlib.md5(y.encode()).hexdigest())\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
